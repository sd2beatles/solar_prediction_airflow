[2022-11-20 11:39:08,524] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: etl_solar.load_files.extract_weather_info scheduled__2022-11-19T00:00:00+00:00 [queued]>
[2022-11-20 11:39:08,539] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: etl_solar.load_files.extract_weather_info scheduled__2022-11-19T00:00:00+00:00 [queued]>
[2022-11-20 11:39:08,540] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2022-11-20 11:39:08,540] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2022-11-20 11:39:08,540] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2022-11-20 11:39:08,566] {taskinstance.py:1377} INFO - Executing <Task(PythonOperator): load_files.extract_weather_info> on 2022-11-19 00:00:00+00:00
[2022-11-20 11:39:08,572] {standard_task_runner.py:52} INFO - Started process 116 to run task
[2022-11-20 11:39:08,575] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'etl_solar', 'load_files.extract_weather_info', 'scheduled__2022-11-19T00:00:00+00:00', '--job-id', '20', '--raw', '--subdir', 'DAGS_FOLDER/dag_preprocess.py', '--cfg-path', '/tmp/tmpppja1bca', '--error-file', '/tmp/tmp4rymx7u6']
[2022-11-20 11:39:08,577] {standard_task_runner.py:80} INFO - Job 20: Subtask load_files.extract_weather_info
[2022-11-20 11:39:08,594] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/***/configuration.py:470: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)

[2022-11-20 11:39:08,652] {task_command.py:369} INFO - Running <TaskInstance: etl_solar.load_files.extract_weather_info scheduled__2022-11-19T00:00:00+00:00 [running]> on host 82103fd5e777
[2022-11-20 11:39:08,752] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=etl_solar
AIRFLOW_CTX_TASK_ID=load_files.extract_weather_info
AIRFLOW_CTX_EXECUTION_DATE=2022-11-19T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-11-19T00:00:00+00:00
[2022-11-20 11:39:08,765] {base.py:68} INFO - Using connection ID 'mysql_solar' for task execution.
[2022-11-20 11:39:08,798] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 171, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 189, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/preprocess/connection.py", line 13, in inner_function
    return function(conn,cursor,**kwargs)
TypeError: save_data() got an unexpected keyword argument 'conf'
[2022-11-20 11:39:08,815] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=etl_solar, task_id=load_files.extract_weather_info, execution_date=20221119T000000, start_date=20221120T113908, end_date=20221120T113908
[2022-11-20 11:39:08,829] {standard_task_runner.py:97} ERROR - Failed to execute job 20 for task load_files.extract_weather_info (save_data() got an unexpected keyword argument 'conf'; 116)
[2022-11-20 11:39:08,869] {local_task_job.py:156} INFO - Task exited with return code 1
[2022-11-20 11:39:08,914] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-11-20 11:44:47,293] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: etl_solar.load_files.extract_weather_info scheduled__2022-11-19T00:00:00+00:00 [queued]>
[2022-11-20 11:44:47,313] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: etl_solar.load_files.extract_weather_info scheduled__2022-11-19T00:00:00+00:00 [queued]>
[2022-11-20 11:44:47,314] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2022-11-20 11:44:47,315] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2022-11-20 11:44:47,315] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2022-11-20 11:44:47,335] {taskinstance.py:1377} INFO - Executing <Task(PythonOperator): load_files.extract_weather_info> on 2022-11-19 00:00:00+00:00
[2022-11-20 11:44:47,340] {standard_task_runner.py:52} INFO - Started process 179 to run task
[2022-11-20 11:44:47,344] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'etl_solar', 'load_files.extract_weather_info', 'scheduled__2022-11-19T00:00:00+00:00', '--job-id', '38', '--raw', '--subdir', 'DAGS_FOLDER/dag_preprocess.py', '--cfg-path', '/tmp/tmpnc3zad36', '--error-file', '/tmp/tmpeq2tl3xf']
[2022-11-20 11:44:47,345] {standard_task_runner.py:80} INFO - Job 38: Subtask load_files.extract_weather_info
[2022-11-20 11:44:47,364] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/***/configuration.py:470: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)

[2022-11-20 11:44:47,424] {task_command.py:369} INFO - Running <TaskInstance: etl_solar.load_files.extract_weather_info scheduled__2022-11-19T00:00:00+00:00 [running]> on host 82103fd5e777
[2022-11-20 11:44:47,513] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=etl_solar
AIRFLOW_CTX_TASK_ID=load_files.extract_weather_info
AIRFLOW_CTX_EXECUTION_DATE=2022-11-19T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-11-19T00:00:00+00:00
[2022-11-20 11:44:47,528] {base.py:68} INFO - Using connection ID 'mysql_solar' for task execution.
[2022-11-20 11:44:47,565] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 171, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 189, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/preprocess/connection.py", line 13, in inner_function
    return function(conn,cursor,**kwargs)
TypeError: save_data() got multiple values for argument 'conn'
[2022-11-20 11:44:47,640] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=etl_solar, task_id=load_files.extract_weather_info, execution_date=20221119T000000, start_date=20221120T114447, end_date=20221120T114447
[2022-11-20 11:44:47,662] {standard_task_runner.py:97} ERROR - Failed to execute job 38 for task load_files.extract_weather_info (save_data() got multiple values for argument 'conn'; 179)
[2022-11-20 11:44:47,677] {local_task_job.py:156} INFO - Task exited with return code 1
[2022-11-20 11:44:47,732] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-11-20 11:49:26,511] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: etl_solar.load_files.extract_weather_info scheduled__2022-11-19T00:00:00+00:00 [queued]>
[2022-11-20 11:49:26,524] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: etl_solar.load_files.extract_weather_info scheduled__2022-11-19T00:00:00+00:00 [queued]>
[2022-11-20 11:49:26,525] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2022-11-20 11:49:26,525] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2022-11-20 11:49:26,525] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2022-11-20 11:49:26,546] {taskinstance.py:1377} INFO - Executing <Task(PythonOperator): load_files.extract_weather_info> on 2022-11-19 00:00:00+00:00
[2022-11-20 11:49:26,552] {standard_task_runner.py:52} INFO - Started process 221 to run task
[2022-11-20 11:49:26,556] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'etl_solar', 'load_files.extract_weather_info', 'scheduled__2022-11-19T00:00:00+00:00', '--job-id', '50', '--raw', '--subdir', 'DAGS_FOLDER/dag_preprocess.py', '--cfg-path', '/tmp/tmpioe5otwi', '--error-file', '/tmp/tmpcq5ddic2']
[2022-11-20 11:49:26,558] {standard_task_runner.py:80} INFO - Job 50: Subtask load_files.extract_weather_info
[2022-11-20 11:49:26,580] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/***/configuration.py:470: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)

[2022-11-20 11:49:26,640] {task_command.py:369} INFO - Running <TaskInstance: etl_solar.load_files.extract_weather_info scheduled__2022-11-19T00:00:00+00:00 [running]> on host 82103fd5e777
[2022-11-20 11:49:26,748] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=etl_solar
AIRFLOW_CTX_TASK_ID=load_files.extract_weather_info
AIRFLOW_CTX_EXECUTION_DATE=2022-11-19T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-11-19T00:00:00+00:00
[2022-11-20 11:49:26,763] {base.py:68} INFO - Using connection ID 'mysql_solar' for task execution.
[2022-11-20 11:49:26,809] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/sql.py", line 2056, in execute
    cur.execute(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/MySQLdb/cursors.py", line 206, in execute
    res = self._query(query)
  File "/home/airflow/.local/lib/python3.7/site-packages/MySQLdb/cursors.py", line 319, in _query
    db.query(q)
  File "/home/airflow/.local/lib/python3.7/site-packages/MySQLdb/connections.py", line 254, in query
    _mysql.connection.query(self, query)
MySQLdb._exceptions.ProgrammingError: (1146, "Table 'solardb.wather_info' doesn't exist")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 171, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 189, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/preprocess/connection.py", line 15, in inner_function
    return function(**kwargs)
  File "/opt/airflow/dags/groups/group_loads.py", line 24, in save_data
    sql_data=pd.read_sql_query(context['params']['sql'],context['conn'])
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/sql.py", line 443, in read_sql_query
    dtype=dtype,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/sql.py", line 2116, in read_query
    cursor = self.execute(*args)
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/sql.py", line 2068, in execute
    raise ex from exc
pandas.io.sql.DatabaseError: Execution failed on sql 'SELECT * FROM wather_info': (1146, "Table 'solardb.wather_info' doesn't exist")
[2022-11-20 11:49:26,830] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=etl_solar, task_id=load_files.extract_weather_info, execution_date=20221119T000000, start_date=20221120T114926, end_date=20221120T114926
[2022-11-20 11:49:26,843] {standard_task_runner.py:97} ERROR - Failed to execute job 50 for task load_files.extract_weather_info (Execution failed on sql 'SELECT * FROM wather_info': (1146, "Table 'solardb.wather_info' doesn't exist"); 221)
[2022-11-20 11:49:26,889] {local_task_job.py:156} INFO - Task exited with return code 1
[2022-11-20 11:49:26,929] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-11-20 11:50:58,392] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: etl_solar.load_files.extract_weather_info scheduled__2022-11-19T00:00:00+00:00 [queued]>
[2022-11-20 11:50:58,414] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: etl_solar.load_files.extract_weather_info scheduled__2022-11-19T00:00:00+00:00 [queued]>
[2022-11-20 11:50:58,415] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2022-11-20 11:50:58,415] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2022-11-20 11:50:58,416] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2022-11-20 11:50:58,454] {taskinstance.py:1377} INFO - Executing <Task(PythonOperator): load_files.extract_weather_info> on 2022-11-19 00:00:00+00:00
[2022-11-20 11:50:58,462] {standard_task_runner.py:52} INFO - Started process 263 to run task
[2022-11-20 11:50:58,469] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'etl_solar', 'load_files.extract_weather_info', 'scheduled__2022-11-19T00:00:00+00:00', '--job-id', '62', '--raw', '--subdir', 'DAGS_FOLDER/dag_preprocess.py', '--cfg-path', '/tmp/tmp4_tfkrcc', '--error-file', '/tmp/tmp8yjghadx']
[2022-11-20 11:50:58,471] {standard_task_runner.py:80} INFO - Job 62: Subtask load_files.extract_weather_info
[2022-11-20 11:50:58,502] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/***/configuration.py:470: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)

[2022-11-20 11:50:58,609] {task_command.py:369} INFO - Running <TaskInstance: etl_solar.load_files.extract_weather_info scheduled__2022-11-19T00:00:00+00:00 [running]> on host 82103fd5e777
[2022-11-20 11:50:58,739] {taskinstance.py:1571} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=etl_solar
AIRFLOW_CTX_TASK_ID=load_files.extract_weather_info
AIRFLOW_CTX_EXECUTION_DATE=2022-11-19T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-11-19T00:00:00+00:00
[2022-11-20 11:50:58,756] {base.py:68} INFO - Using connection ID 'mysql_solar' for task execution.
[2022-11-20 11:50:58,822] {python.py:173} INFO - Done. Returned value was: None
[2022-11-20 11:50:58,844] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=etl_solar, task_id=load_files.extract_weather_info, execution_date=20221119T000000, start_date=20221120T115058, end_date=20221120T115058
[2022-11-20 11:50:58,891] {local_task_job.py:156} INFO - Task exited with return code 0
[2022-11-20 11:50:58,970] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
